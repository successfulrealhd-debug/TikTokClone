# OpenHands local config for free + offline use with Ollama
# Place this file in: .openhands/config.toml

[core]
workspace_base = "./workspace"

[llm]
provider = "ollama"
model = "qwen3:8b"
api_base = "http://localhost:11434"   # Ollama default server

[coding]
enabled = true
auto_commit = true
commit_message = "OpenHands AutoCoder update"

[chat]
enabled = true
system_prompt = "You are OpenHands, an autonomous coder. Generate complete solutions, not snippets."
